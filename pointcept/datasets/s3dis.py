import os
import glob
import numpy as np
import torch
from copy import deepcopy
from torch.utils.data import Dataset
from collections.abc import Sequence

from pointcept.utils.logger import get_root_logger
from pointcept.utils.cache import shared_dict
from .builder import DATASETS
from .transform import Compose, TRANSFORMS


@DATASETS.register_module()
class S3DISDataset(Dataset):
    def __init__(
        self,
        split=("Area_1", "Area_2", "Area_3", "Area_4", "Area_6"),
        data_root="data/s3dis",
        transform=None,
        test_mode=False,
        test_cfg=None,
        cache=False,
        loop=1,
    ):
        super(S3DISDataset, self).__init__()
        self.data_root = data_root
        self.split = split
        self.transform = Compose(transform)
        self.cache = cache
        self.loop = (
            loop if not test_mode else 1
        )  # force make loop = 1 while in test mode
        self.test_mode = test_mode
        self.test_cfg = test_cfg if test_mode else None

        if test_mode:
            self.test_voxelize = TRANSFORMS.build(self.test_cfg.voxelize)
            self.test_crop = (
                TRANSFORMS.build(self.test_cfg.crop) if self.test_cfg.crop else None
            )
            self.post_transform = Compose(self.test_cfg.post_transform)
            self.aug_transform = [Compose(aug) for aug in self.test_cfg.aug_transform]

        self.data_list = self.get_data_list()
        logger = get_root_logger()
        logger.info(
            "Totally {} x {} samples in {} set.".format(
                len(self.data_list), self.loop, split
            )
        )

    def get_data_list(self):
        if isinstance(self.split, str):
            data_list = glob.glob(os.path.join(self.data_root, self.split, "*.pth"))
        elif isinstance(self.split, Sequence):
            data_list = []
            for split in self.split:
                data_list += glob.glob(os.path.join(self.data_root, split, "*.pth"))
        else:
            raise NotImplementedError
        return data_list
        
    def get_data(self, idx):
        data_path = self.data_list[idx % len(self.data_list)]
        if not self.cache:
            data = torch.load(data_path)
        else:
            data_name = data_path.replace(os.path.dirname(self.data_root), "").split(".")[0]
            cache_name = "pointcept" + data_name.replace(os.path.sep, "-")
            data = shared_dict(cache_name)

        name = os.path.basename(self.data_list[idx % len(self.data_list)]).split("_")[0].replace("R", " r")
        coord = data["coord"]
        scene_id = data_path

        if "semantic_gt" in data.keys():
            segment = data["semantic_gt"].reshape([-1])
        else:
            segment = np.ones(coord.shape[0]) * -1

        if "instance_gt" in data.keys():
            instance = data["instance_gt"].reshape([-1])
        else:
            instance = np.ones(coord.shape[0]) * -1

        data_dict = dict(
            name=name,
            coord=coord,
            segment=segment, 
            instance=instance,
            scene_id=scene_id,
        )

        return data_dict

    def get_data_name(self, idx):
        return os.path.basename(self.data_list[idx % len(self.data_list)]).split(".")[0]

    def prepare_train_data(self, idx):
        # load data
        data_dict = self.get_data(idx)
        data_dict = self.transform(data_dict)
        return data_dict

    ############################################################
    ###-prepare_test_data function for semantic segmentation-###
    ############################################################
    def prepare_test_data(self, idx):
        # load data
        data_dict = self.get_data(idx)
        data_dict = self.transform(data_dict)
        result_dict = dict(
            segment=data_dict.pop("segment"), name=self.get_data_name(idx)
        )
        if "origin_segment" in data_dict:
            assert "inverse" in data_dict
            result_dict["origin_segment"] = data_dict.pop("origin_segment")
            result_dict["inverse"] = data_dict.pop("inverse")

        data_dict_list = []
        for aug in self.aug_transform:
            data_dict_list.append(aug(deepcopy(data_dict)))

        fragment_list = []
        for data in data_dict_list:
            if self.test_voxelize is not None:
                data_part_list = self.test_voxelize(data)
            else:
                data["index"] = np.arange(data["coord"].shape[0])
                data_part_list = [data]
            for data_part in data_part_list:
                if self.test_crop is not None:
                    data_part = self.test_crop(data_part)
                else:
                    data_part = [data_part]
                fragment_list += data_part

        for i in range(len(fragment_list)):
            fragment_list[i] = self.post_transform(fragment_list[i])
        result_dict["fragment_list"] = fragment_list
        return result_dict

    ############################################################
    ###-prepare_test_data function for instance segmentation-###
    ############################################################
    # def prepare_test_data(self, idx):
    #     import copy
    #     data_dict = self.get_data(idx)
    #     # print("Original keys in data_dict:", list(data_dict.keys()))
    #     data_dict = self.transform(data_dict)
    #     # print("keys in data_dict after transform:", list(data_dict.keys()))

    #     result_dict = dict(name=self.get_data_name(idx))

    #     data_dict_list = []
    #     for aug in self.aug_transform:
    #         data_dict_list.append(aug(copy.deepcopy(data_dict)))

    #     fragment_list = []
    #     for data_ in data_dict_list:
    #         if self.test_voxelize is not None:
    #             data_part_list = self.test_voxelize(data_)
    #         else:
    #             data_["index"] = np.arange(data_["coord"].shape[0])
    #             data_part_list = [data_]

    #         final_parts = []
    #         for data_part in data_part_list:
    #             if self.test_crop is not None:
    #                 cropped_parts = self.test_crop(data_part)
    #             else:
    #                 cropped_parts = [data_part]
    #             final_parts.extend(cropped_parts)

    #         fragment_list.extend(final_parts)

    #     # 5) post_transform
    #     for i in range(len(fragment_list)):
    #         fragment_list[i] = self.post_transform(fragment_list[i])

    #     first_frag = fragment_list[0]

    #     if "origin_segment" in first_frag:
    #         result_dict["origin_segment"] = first_frag["origin_segment"]
    #     if "inverse" in first_frag:
    #         result_dict["inverse"] = first_frag["inverse"]

    #     if "segment" in first_frag:
    #         result_dict["segment"] = first_frag["segment"]

    #     result_dict["fragment_list"] = fragment_list

    #     return result_dict

    

    def __getitem__(self, idx):
        if self.test_mode:
            return self.prepare_test_data(idx)
        else:
            return self.prepare_train_data(idx)

    def __len__(self):
        return len(self.data_list) * self.loop